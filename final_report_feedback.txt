Team 2: 90/100

Team 2 did a solid job on the final report, which includes a discussion of the algorithm used, the choice of programming languages, functionality, infrastructure used such as ANTLR and Spring, a UML diagram showing the high-level architecture of the system, some snapshots from the UI, and a good discussion of the limitations of the plagiarism detector and lessons learned from doing the project. The report is well-organized and the quality of the writing is very high. That said, there is some room for improvement, see comments below:
  - When you use an existing algorithm such as the winnowing algorithm, you should always acknowledge the source and provide a citation.
  - It is clear that the team lost significant time due to a steep learning curve with infrastructure, and perhaps setting too ambitious a goal at the outset. It is good to see that, in the end, a positive result was achieved.
  - As mentioned, a significant limitation of the adopted line-based comparison approach is that renamings of variables cannot be detected.  One solution for that would have been to systematically rename all variables in both files under consideration (another group took this approach and referred to this as using “canonical names”).
  - The algorithm is presented well, but it is always hard to fully understand an algorithm if it is presented in prose. A better solution would have been to use some form of pseudo-code. That would also make it easier to discuss the complexity of the algorithm as it would be evident from the structure of the nested loops.
